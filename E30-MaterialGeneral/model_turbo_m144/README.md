---
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- generated_from_trainer
- dataset_size:5636
- loss:MultipleNegativesRankingLoss
widget:
- source_sentence: GEL ULTRASONS AQUASONIC 250g
  sentences:
  - 'INTERVENCIONSIME PERIF√àRIC VASCULAR: CAT√àTER BAL√ì ANGIOPL√ÄSTIA (ATP), CAT√àTER
    ANGIOGR√ÄFIC/ MICROCAT√àTER/ CAT√àTER GUIA, INTERPERIFVAS:CAT√àTER ANGIO/DIAG ATP,
    5FR ST / 65CM / GUIA 0,035  FLUX M√ÄX 20 ML/SEC'
  - AGULLES PER PUNCI√ì LUMBAR, EQUIPS PER ANEST√àSIA SIMPLE I COMBINADA ESPINAL/EPIDURAL,
    DISPOSITIU PER A CONTROL DE LA PRESSI√ì D'INJECCI√ì EN PROCEDIMENTS DE BLOQUEIG
    NERVI√ìS PERIF√àRIC, AMB 3 RANGS DE COLORS <15PSI, 15-20PSI I >20PSI. EST√âRIL, D'UN
    SOL √öS.
  - ALTRES ACCESSORIS PER A EL√àCTRODES, GELS CONDUCTORS, GEL CONDUCTOR ULTRASONS EEG
    ELECTRO ENCEFALOGRAMES, POTENCIALS EVOCATS MOTORS, HIPOAL.LERG√àNIC, BACTERIOST√ÄTIC.
    PRESENTACI√ì POT 250G +/-10% (B*)BAIXA PER DUPLICITAT,VEURE CODI 30001457
- source_sentence: SONDA GASTRO MIC-G 14F PEDIATRIC
  sentences:
  - 'INTERVENCIONSIME PERIF√àRIC VASCULAR: CAT√àTER BAL√ì ANGIOPL√ÄSTIA (ATP), CAT√àTER
    ANGIOGR√ÄFIC/ MICROCAT√àTER/ CAT√àTER GUIA, INTERPERIFVAS:CAT√àTER ANGIO/DIAG ATP,
    CAT√àTER ANGIOGR√ÄFIC D''ALT FLUX, 5X90CM DE LONGITUD, NO TRENAT, COMPATIBLE AMB
    GUIA DE 0,035". PIGTAIL.REF:7602-51'
  - RECOLLIDA D'ORINA I INCONTIN√àNCIA, DISPOSITIUS PER A INCONTIN√àNCIA URIN√ÄRIA, COL¬∑LECTOR
    DE L√ÄTEX AUTOADHESIU PER A INCONTIN√àNCIA URIN√ÄRIA MASCULINA DE 25 (+/-1) MM√ò.
    HIPOAL¬∑LERG√àNIC, ENV√ÄS INDIVIDUAL.
  - NUTRICI√ì ENTERAL, SONDA GASTROSTOMIA I EXTENSIONS AMB CONNEXI√ì ENFIT, SONDA GASTR
    BAIX PERFIL TIPUS BOT√ì,SILIC GRAU M√àDIC,14F,√òESTOMA2,0CM,BAL√ì RET INT,PUNTA REC
    P/BAL√ì,L√çNIA RX,ALLARG RECT ENFIT,ALLARG ANGUL ENFIT,GUIA SELDINGER,80CM+/-5CM,INTROD,XERENFIT
    60ML,XER LUER,GEL LUBR,GASSES,ENFIT,N/L√ÄTEX,DEHP I BPA,EST√àRIL,1√öS
- source_sentence: INTRODUCTOR  A/VALVULA HEMOST 8 F 11CM
  sentences:
  - CAT√àTERS INTRAVASCULARS PER A PERFUSI√ì, CAT√àTER INTRAVASCULAR PUNCI√ì VENOSA PERIFERICA
    CURT, CAT√àTER IV PERIF P/INFUSI√ì M√ÄX 22,41BARS(325PSI),PUR,RADIOPAC,2LLUM,L√çNIA
    MITJANA 4F/√ò19G/21GX20(+/-1)CM LLARG,MARQUES PENET,ALETES,CAT√àTER CURT BSA,GUIA50(+/-2)CM,AG
    INTROD,MICROINTROD,INTROD,ESTABILIT,CLAMP,TAP,XERING,BISTU BSA,CINT M√àT.EST,S/L√ÄTEX,1√öS
  - CAT√àTERS INTRAVASCULARS PER A PERFUSI√ì, CAT√àTER INTRAVASCULAR PUNCI√ì VENOSA PERIFERICA
    CURT, CAT IV PERIF√àRIC PUR, ALTA PRESSI√ì 300 PSI,RADIOOP,1LLUM L√çNIA MITJA 4FX20(+/-1)CM,MARQUES
    PENET,ALETES,GUIA,AGULLA,INTRODUCTOR,CLAMP,TAP,BIOCONNECTOR,XERINGA,BISTUR√ç BSA+M√ÄNEC,TORNIQUET,CINTA
    M√àTRICA,GUILLOTINA,CLAVA-AGULLES SEGURETAT.EST,S/L√ÄTEX,1√öS
  - INTRODUCTORS DE CAT√àTERS, INTRODUCTOR ARTERIAL ACC√âS FEMORAL, EQUIP INTRODUCTOR
    RADIAL FEMORAL, 5FX40CM DE LONGITUD DE LA GUIA, V√ÅLVULA HEMOST√ÄTICA, CLAU 3V I
    DILATADOR PER A GUIA 0,035", TAMANY AGULLA 18G, TIPUS DE PUNTA J O RECTA. BALTON.
    REF; INT5F
- source_sentence: CATETER ESTIMULACIO BIPOLAR 6F 110CM
  sentences:
  - 'INTERVENCIONSIME PERIF√àRIC VASCULAR: CAT√àTER BAL√ì ANGIOPL√ÄSTIA (ATP), CAT√àTER
    ANGIOGR√ÄFIC/ MICROCAT√àTER/ CAT√àTER GUIA, INTERPERIFVAS:CAT√àTER ANGIO/DIAG ATP,
    CAT√àTER ANGIOGR√ÄFIC ATP SELECTIU VISCERAL,TRENAT,NO HIDROFILIC,√ò5FX65CM,CORBA
    TIPUS RDC,SEGMENT DISTAL 2 ORIFICIS,CABAL INFUSI√ì M√ÄXIM 20ML/SEG,PER A GUIA 0,035",EST√àRIL,UN
    SOL √öS. IMAGER II,REF:M001314891'
  - AGULLES I EQUIPS PER A BI√íPSIA, AGULLES BI√íPSIA  I LOCALITZACI√ì MAM√ÄRIA, MARCADOR
    DE LESIONS MAM√ÄRIES PER A LOCALITZACI√ì MAGN√àTICA, ANTI-MIGRACI√ì, MIDA 1MM X 5+/-1MM,
    AMB AGULLA INTRODUCTORA 18GX 7CM +/- 0,3CM. EST√àRIL, UN SOL  √öS.
  - NUTRICI√ì ENTERAL, SONDA GASTROSTOMIA I EXTENSIONS AMB CONNEXI√ì ENFIT, SONDA GAST
    ‚ÜìPERF 14F/2,5CMENFIT
- source_sentence: SET PICC 2L 5F 55CM
  sentences:
  - AGULLES I EQUIPS PER A BI√íPSIA, AGULLES BI√íPSIA  I LOCALITZACI√ì MAM√ÄRIA, SISTEMA
    REFLECTOR AMB LLIURAMENT D'AGULLA DE 7,5CM PER A LOCALITZACI√ì I EXTIRPACI√ì DE
    TEIXIT MAMARI. COMPATIBLE AMB SISTEMA GUIA SCOUT O EQUIVALENT.
  - RECOLLIDA DE MOSTRES, ALTRES RECIPIENTS PER RECOLLIR MOSTRES, RECIPIENT R√çGID/CANISTER  PER
    A  RECOLLIDA DE FLUIDS I DE MOSTRES, NO EST√àRIL, D'UN SOL √öS. COMPATIBLE MORCELADOR
    HISTEROSCOP SISTEMA OHS TRUCLEAR O EQUIVALENT.
  - CAT√àTERS INTRAVASCULARS PER A PERFUSI√ì, CAT√àTER INTRAVASCULAR CENTRAL D'INSERCI√ì
    PERIF√àRICA (PICC), CAT√àTER PICC,1 LLUM,PUR,RADIOPAC,√ò5F,LLARG 60+/-2CM MARQUES
    PENETRACI√ì,PE√áA"T"PROXIMAL INJECCI√ì INTRODUCTOR A/GUIA 67+/-2CM FLUOROSC√íPIA,BIOCONNECTOR,CINTA
    M√àTRICA,DISPOSITIU FIXACI√ì,CLAMP,XERINGA,BISTUR√ç,ETIQUETES  TRA√áABILITAT.EST,S/L√ÄTEX/FTALATS,1√öS
pipeline_tag: sentence-similarity
library_name: sentence-transformers
---

# SentenceTransformer

This is a [sentence-transformers](https://www.SBERT.net) model trained. It maps sentences & paragraphs to a 384-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
<!-- - **Base model:** [Unknown](https://huggingface.co/unknown) -->
- **Maximum Sequence Length:** 256 tokens
- **Output Dimensionality:** 384 dimensions
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the ü§ó Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    'SET PICC 2L 5F 55CM',
    'CAT√àTERS INTRAVASCULARS PER A PERFUSI√ì, CAT√àTER INTRAVASCULAR CENTRAL D\'INSERCI√ì PERIF√àRICA (PICC), CAT√àTER PICC,1 LLUM,PUR,RADIOPAC,√ò5F,LLARG 60+/-2CM MARQUES PENETRACI√ì,PE√áA"T"PROXIMAL INJECCI√ì INTRODUCTOR A/GUIA 67+/-2CM FLUOROSC√íPIA,BIOCONNECTOR,CINTA M√àTRICA,DISPOSITIU FIXACI√ì,CLAMP,XERINGA,BISTUR√ç,ETIQUETES  TRA√áABILITAT.EST,S/L√ÄTEX/FTALATS,1√öS',
    "AGULLES I EQUIPS PER A BI√íPSIA, AGULLES BI√íPSIA  I LOCALITZACI√ì MAM√ÄRIA, SISTEMA REFLECTOR AMB LLIURAMENT D'AGULLA DE 7,5CM PER A LOCALITZACI√ì I EXTIRPACI√ì DE TEIXIT MAMARI. COMPATIBLE AMB SISTEMA GUIA SCOUT O EQUIVALENT.",
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 384]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset

* Size: 5,636 training samples
* Columns: <code>sentence_0</code> and <code>sentence_1</code>
* Approximate statistics based on the first 1000 samples:
  |         | sentence_0                                                                        | sentence_1                                                                           |
  |:--------|:----------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------|
  | type    | string                                                                            | string                                                                               |
  | details | <ul><li>min: 6 tokens</li><li>mean: 13.71 tokens</li><li>max: 25 tokens</li></ul> | <ul><li>min: 20 tokens</li><li>mean: 106.98 tokens</li><li>max: 169 tokens</li></ul> |
* Samples:
  | sentence_0                                            | sentence_1                                                                                                                                                                                                                                                                                                                                                               |
  |:------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | <code>FILTRE BACTERI VIRUS C. ANEST. PEDIATRIC</code> | <code>FILTRES, FILTRES P/RETENCI√ì BACT√àRIS I VIRUS EN CIRCUITS VENTILACI√ì, FILTRE MEC√ÄNIC ANTIBACTERI√Ä/ANTIV√çRIC VM, MEMBRANA HIDROF√íBICA, PRESA CAPNOGRAFIA, FILTRACI√ì 99,9999% (BACTERIS), 99,999% (VIRUS). VT RECOMANAT300-1500 ML, ESPAI MORT 92(+/-5)ML, PES 47(+/-5)G, CONN 22M/15F-22F/15M. LLIURE DE L√ÄTEX, EST√àRIL, UN SOL √öS.</code>                           |
  | <code>CATETER AMB CORBA FLUX 5F 100CM</code>          | <code>INTERVENCIONSIME PERIF√àRIC VASCULAR: CAT√àTER BAL√ì ANGIOPL√ÄSTIA (ATP), CAT√àTER ANGIOGR√ÄFIC/ MICROCAT√àTER/ CAT√àTER GUIA, INTERPERIFVAS:CAT√àTER ANGIO/DIAG ATP, 5FR ST / 65CM / GUIA 0,038 FLUX M√ÄX 20 ML/SEC</code>                                                                                                                                                  |
  | <code>CATETER ESTIMULACIO BIPOLAR 6F 110CM</code>     | <code>INTERVENCIONSIME PERIF√àRIC VASCULAR: CAT√àTER BAL√ì ANGIOPL√ÄSTIA (ATP), CAT√àTER ANGIOGR√ÄFIC/ MICROCAT√àTER/ CAT√àTER GUIA, INTERPERIFVAS:CAT√àTER ANGIO/DIAG ATP, CAT√àTER ANGIOGR√ÄFIC DE 5FX65CM, MULTI√öS, NO HIDROFILIC, SENSE ORIFICIS, AMB PUNTA RADIOOPACA RIM. PER A GUIA DE 0,038". EST√àRIL. SELECTIUS VISCERALS MULTIPROPOSIT; REF: HNB5.0-38-65-P-NS-RIM</code> |
* Loss: [<code>MultipleNegativesRankingLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss) with these parameters:
  ```json
  {
      "scale": 20.0,
      "similarity_fct": "cos_sim"
  }
  ```

### Training Hyperparameters
#### Non-Default Hyperparameters

- `per_device_train_batch_size`: 64
- `per_device_eval_batch_size`: 64
- `num_train_epochs`: 1
- `multi_dataset_batch_sampler`: round_robin

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: no
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 64
- `per_device_eval_batch_size`: 64
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `torch_empty_cache_steps`: None
- `learning_rate`: 5e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1
- `num_train_epochs`: 1
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.0
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `use_ipex`: False
- `bf16`: False
- `fp16`: False
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: None
- `hub_always_push`: False
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `include_for_metrics`: []
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `dispatch_batches`: None
- `split_batches`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: False
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `use_liger_kernel`: False
- `eval_use_gather_object`: False
- `average_tokens_across_devices`: False
- `prompts`: None
- `batch_sampler`: batch_sampler
- `multi_dataset_batch_sampler`: round_robin

</details>

### Framework Versions
- Python: 3.12.7
- Sentence Transformers: 3.4.1
- Transformers: 4.49.0
- PyTorch: 2.6.0+cpu
- Accelerate: 1.5.2
- Datasets: 3.4.0
- Tokenizers: 0.21.1

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

#### MultipleNegativesRankingLoss
```bibtex
@misc{henderson2017efficient,
    title={Efficient Natural Language Response Suggestion for Smart Reply},
    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},
    year={2017},
    eprint={1705.00652},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->